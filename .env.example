# mull configuration
# Copy this to .env and adjust as needed

# LLM API endpoint (OpenAI-compatible)
# Examples:
#   LM Studio:  http://localhost:1234/v1
#   Ollama:     http://localhost:11434/v1
#   llama.cpp:  http://localhost:8080/v1
MULL_API_BASE=http://localhost:1234/v1

# API key (most local servers don't need this)
MULL_API_KEY=not-needed

# Model name (depends on what you have loaded)
MULL_MODEL=llama3
