ğŸ”§ PromptForge
A fully local, open-source LLM prompt analysis and critique pipeline â€” for deep reasoning, better answers, and zero API costs.

ğŸ’¡ What It Does
PromptForge is a local AI-powered system that:

Classifies user prompts (simple vs. complex)

Routes them to the right LLM processing pipeline

Applies a step-by-step critique and revision process

Returns sharper, more thoughtful responses â€” automatically

All of this runs 100% locally using open-source models and tools like Ollama, LM Studio, and Streamlit. No API keys. No cloud dependencies.

ğŸ§± Features
ğŸ” Auto classification of prompt type (simple vs full-pipeline)

ğŸ› ï¸ Modular pipeline with critique, persona shift, revision, and meta-reflection

âš™ï¸ Streamlit UI for simple UX with labeled outputs and clean flow

ğŸ§  Model-agnostic design â€” works with any local model (Mistral, Phi, TinyLlama, etc.)

ğŸ—‚ï¸ All outputs saved as structured markdown logs

ğŸ–¼ Example Use Case
Prompt: â€œWhat should I consider before launching a SaaS business?â€

PromptForge automatically:

Detects it's a strategic (complex) question

Generates a first answer

Critiques it from multiple perspectives

Revises and refines the response

Surfaces unresolved tensions

Summarizes how the final output evolved

All visible step-by-step.

ğŸ–¥ï¸ Tech Stack
Tool	Purpose
Python 3.9+	Core logic + LLM routing
Streamlit	Interactive frontend
Ollama / LM Studio	Model hosting (GGUF)
TinyLlama / Mistral / Phi	Local language models
Markdown	Output formatting & logs

ğŸš€ Quick Start
1. Clone the repo
bash
Copy
Edit
git clone https://github.com/HCS412/local_llm_runner.git
cd local_llm_runner
2. Install dependencies
bash
Copy
Edit
pip install -r requirements.txt
3. Start the Streamlit app
bash
Copy
Edit
streamlit run app.py
4. Load a model locally
Make sure LM Studio or Ollama is running a supported model (like tinyllama, mistral, or phi).

ğŸ§ª Prompt Pipeline
The system dynamically chooses between:

Type	Description
simple	Direct factual lookup (e.g., â€œWho was the first president?â€)
full_pipeline	Complex reasoning, reflection, or opinion-based prompts

The full pipeline includes:

Initial generation

Critique (reasoning flaws, biases)

Expansion of depth/context

Persona shift / rephrasing

Revised answer

Final critique + tension surfacing

Summary and log

ğŸ“ Directory Structure
bash
Copy
Edit
â”œâ”€â”€ app.py                     # Streamlit frontend
â”œâ”€â”€ main.py                    # Core CLI runner
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ prompt_router.py       # Determines simple vs full
â”‚   â”œâ”€â”€ utils_prompt_classifier.py # Auto prompt classifier
â”‚   â”œâ”€â”€ formatting.py          # Markdown formatting
â”‚   â”œâ”€â”€ __init__.py            # Imports
â”œâ”€â”€ logs/                      # Run logs as .md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
ğŸ”§ Configuration Tips
ğŸ§  Swap models via LM Studio or Ollama (just update in main.py)

ğŸ“¦ Add more critique personas by expanding principles.json

âš¡ Improve performance by choosing smaller models or streamlining pipeline stages

ğŸŒ Examples
bash
Copy
Edit
python main.py "How can I launch a startup with no funding?"
python main.py "What does it take to be a great investor?"
python main.py "Why do people still follow controversial public figures?"
ğŸ“Œ Roadmap
 Optional GPT-4 comparison panel

 Ability to toggle critique personas (e.g. teacher, founder, historian)

 FastAPI or local API for app integrations

 Side-by-side LLM benchmarking

 Persistent config settings and memory

ğŸ¤ Contributing
Open to ideas, PRs, and critiques â€” especially from:

ML builders

Prompt engineers

Educators

Philosophers

Curious hackers

ğŸ“£ License
MIT License.
Build, remix, explore. Just donâ€™t lock it behind a paywall.

