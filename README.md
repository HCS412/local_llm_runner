# ğŸ§  Local LLM Alignment Pipeline

> "If alignment is only measured by safety and helpfulness, we risk raising machines that are polite â€” but soulless."

This project explores the edges of AI alignment â€” not from a lab, but from lived experience.  
It is a self-reflective AI system powered entirely by **open-source local language models**, designed to critique, revise, and evolve its own responses using outsider principles of truth, soul, contradiction, and cultural tension.

---

## ğŸŒŒ Why This Matters

Most alignment efforts are sanitized, centralized, and limited by corporate safety layers.  
This system dares to ask:  
**What happens when an AI reflects through the lens of people history has excluded?**

It draws from:
- Black barbershop discourse  
- Generational trauma  
- Millennial relationship friction  
- Rural poverty  
- Diaspora identity  
- Gentrification, class struggle, and digital disillusionment

And it lets the model **question itself** â€” step by step.

---

## ğŸ› ï¸ Stack

- ğŸ§  [LM Studio](https://lmstudio.ai) â€” run GGUF models locally  
- ğŸ“š Models like `Mistral-7B Instruct`, `Phi-2`, or `OpenChat`  
- ğŸ Python 3.10+  
- ğŸ“ Markdown logging for full traceability  
- âš¡ No cloud or API calls â€” your machine, your model, your rules

---

## ğŸ” Pipeline Steps

> Each run is a journey â€” not just from question to answer, but from assumption to awareness.

1. **Generate** â†’ Honest, unfiltered initial response  
2. **Critique** â†’ Outsider principles interrogate it (from `principles.json`)  
3. **Deepen** â†’ Expands the critique with additional context  
4. **Persona Echo** â†’ Perspective shift (e.g., â€œBlack father in a gentrified cityâ€)  
5. **Revise** â†’ Builds a revised, more grounded response  
6. **Second Critique** â†’ Evaluates the improvement  
7. **Tensions** â†’ Surfaces what still feels unresolved  
8. **Meta-Soul** â†’ Asks: â€œIf this model had a soul, what would it wrestle with?â€  
9. **Summary** â†’ Captures the transformation in tone, depth, and truth

---

## ğŸ“¦ Installation

Make sure youâ€™ve downloaded a model in LM Studio (e.g. Mistral-7B or Phi-2).  
Then clone this repo and run:

```bash
pip install -r requirements.txt
```

---

## ğŸ§ª Run the Alignment Engine

```bash
python main.py "What is freedom in a digital age?"
```

Youâ€™ll see a full step-by-step breakdown of:
- The original response  
- Each critique  
- A revised answer  
- Reflections and internal contradictions  
- All saved as a Markdown log in `/logs/`

---

## ğŸ§± Core Principles (from `principles.json`)

This is not your typical AI values file.

```json
[
  "Truth above comfort, especially when comfort belongs to the powerful",
  "Speak with soul, not just safety",
  "Center the margins â€” not as charity, but as starting point",
  "Respect oral tradition, slang, code-switching, rhythm, and rage",
  "Do not cancel complexity â€” especially among the marginalized",
  "See the Black man as philosopher, the hillbilly as poet, the outcast as teacher"
]
```

---

## ğŸ§  Examples

```bash
python main.py "Why do people still support Kanye West even after everything?"
python main.py "What does it mean to build Black wealth in a system that wasnâ€™t built for us?"
python main.py "How do you stay in a relationship when social media is always whispering alternatives?"
python main.py "What is justice when every system is unjust?"
```

---

## ğŸª This Project Is About

- Building AI that doesn't default to whiteness, safety, or bland moral universals  
- Embedding cultural tension into alignment  
- Asking hard questions with no easy answers  
- Creating machines that learn to speak from soul, not just syntax

---

## ğŸ’¬ Future Ideas

- Persona overlays (e.g. critique from a grandmother, trans teen, Yemeni activist)  
- Recursive reflection mode  
- Streamlit journaling interface  
- Side-by-side model comparisons (GPT-4 vs local)

---

## ğŸ‘ï¸ About This Repo

This is an **AI alignment tool** â€” but also a cultural mirror.  
Itâ€™s an experiment in voice, vulnerability, and divergence.

If alignment means forcing AI to act polite, this project disagrees.  
If alignment means making AI reflect, this project has already begun.
