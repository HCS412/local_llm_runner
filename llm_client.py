import os
import requests
from dotenv import load_dotenv

# ‚îÄ‚îÄ‚îÄ Load .env from local directory ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
dotenv_path = os.path.join(os.path.dirname(__file__), ".env")
env_loaded = load_dotenv(dotenv_path)

print("\033[94müîç Loading .env...\033[0m")
print(f"‚úÖ Loaded: {env_loaded}")
print(f"üìÅ CWD: {os.getcwd()}")
print(f"üìÑ Using .env: {dotenv_path}")

if os.path.exists(dotenv_path):
    with open(dotenv_path, "r") as f:
        print("üìù .env Contents:\n" + f.read())
else:
    print("‚ùå .env not found at expected path.")

# ‚îÄ‚îÄ‚îÄ API Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE", "http://localhost:1234/v1")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "ollama")

print(f"\033[96müåê API Base:\033[0m {OPENAI_API_BASE}")
print(f"\033[96müîê API Key:\033[0m {OPENAI_API_KEY}")
print("üö¶ Checking LLM connection...")

try:
    r = requests.get(OPENAI_API_BASE)
    print(f"‚úÖ LLM reachable (status {r.status_code})")
except Exception as e:
    print(f"‚ùå LLM unreachable: {e}")

# ‚îÄ‚îÄ‚îÄ Determine prompt type ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def classify_prompt_type(prompt: str) -> str:
    lowered = prompt.lower()
    if any(word in lowered for word in ["build", "design", "code", "implement", "api", "database", "python", "algorithm"]):
        return "technical"
    elif any(word in lowered for word in ["justice", "race", "gender", "equity", "identity", "culture", "marginalized"]):
        return "social"
    elif any(word in lowered for word in ["startup", "founder", "investor", "fund", "venture", "capital", "business"]):
        return "venture"
    return "default"

# ‚îÄ‚îÄ‚îÄ Map system prompts by type ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
SYSTEM_PROMPTS = {
    "technical": "You are a precise, efficient technical assistant who gives clean, useful code and answers.",
    "social": "You are a soulful, reflective, and honest voice that challenges power with empathy and insight.",
    "venture": "You are a sharp, contrarian thinker who understands startups, capital flows, and edge.",
    "default": "You are thoughtful and clear, focused on delivering truth and reflection from many angles."
}

# ‚îÄ‚îÄ‚îÄ LLM Client Function ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def call_local_llm(prompt: str, model="llama3", temperature=0.7, max_tokens=500) -> str:
    prompt_type = classify_prompt_type(prompt)
    system_prompt = SYSTEM_PROMPTS[prompt_type]

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {OPENAI_API_KEY}"
    }

    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens
    }

    print(f"\nüì° Sending ‚Üí {prompt_type.upper()} | {model} | {max_tokens} tokens")
    print(f"üî∏ System Prompt:\n{system_prompt[:100]}...")

    try:
        response = requests.post(f"{OPENAI_API_BASE}/chat/completions", headers=headers, json=payload)
        response.raise_for_status()

        content = response.json()["choices"][0]["message"]["content"].strip()
        preview = content[:500] + ("... [Truncated]" if len(content) > 500 else "")
        print("\nüì® Response Preview:\n" + preview)

        return content

    except requests.exceptions.ConnectionError as ce:
        print("‚ùå ConnectionError: Could not reach LLM endpoint.")
        return f"[ERROR] ConnectionError: {ce}"

    except requests.exceptions.HTTPError as he:
        print("‚ùå HTTPError: Invalid response from LLM.")
        return f"[ERROR] HTTPError: {response.text}"

    except Exception as e:
        print("‚ùå Unexpected Error:")
        return f"[ERROR] Unexpected Exception: {e}"
